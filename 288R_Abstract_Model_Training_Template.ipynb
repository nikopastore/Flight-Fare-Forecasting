{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yymT4zVRTTU"
      },
      "source": [
        "# Data Load"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procedure"
      ],
      "metadata": {
        "id": "RXdFgw9ZXijn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAI0dO1O5O_G"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runtime_log = []\n",
        "section_flag = 0\n",
        "def log_time():\n",
        "  t = time.time()\n",
        "  runtime_log.append(t)\n",
        "  return t\n",
        "def time_flag(note = 'Process complete!', frum = -2, to = -1, save_flag = False):\n",
        "  log_time()\n",
        "  print(f'\\n{note} ' +\n",
        "        f'({np.floor((runtime_log[to] - runtime_log[frum]) / 60)} minutes and {(runtime_log[to] - runtime_log[frum]) % 60} seconds)')\n",
        "  if save_flag:\n",
        "    return len(runtime_log) - 1"
      ],
      "metadata": {
        "id": "eN7nBXCB9woQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVCPTXrlYgqF"
      },
      "outputs": [],
      "source": [
        "log_time()\n",
        "!pip install kaggle\n",
        "!kaggle datasets download -d dilwong/flightprices\n",
        "time_flag()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ww_yO_q2cZyN"
      },
      "outputs": [],
      "source": [
        "log_time()\n",
        "!unzip -n flightprices.zip\n",
        "time_flag()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECF7PIMN-aV6"
      },
      "outputs": [],
      "source": [
        "log_time()\n",
        "!pip install pyspark\n",
        "time_flag()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7i7Hn6G7ihyw"
      },
      "outputs": [],
      "source": [
        "log_time()\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.functions import *\n",
        "try:\n",
        "  sc.stop()\n",
        "except:\n",
        "  pass\n",
        "sc = SparkContext()\n",
        "sqlContext = SQLContext(sc)\n",
        "time_flag()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zvi4e_DM-bsg"
      },
      "outputs": [],
      "source": [
        "log_time()\n",
        "ss = SparkSession.builder.getOrCreate()\n",
        "time_flag()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9iU0NybPOx2"
      },
      "outputs": [],
      "source": [
        "log_time()\n",
        "df = sqlContext.read.csv('itineraries.csv', header = True)\n",
        "time_flag()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bMIWBCaPeXK"
      },
      "outputs": [],
      "source": [
        "log_time()\n",
        "df.show()\n",
        "time_flag()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "(View runtime of procedure following execution)"
      ],
      "metadata": {
        "id": "hDPuZNYLPh6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "section_flag = time_flag(note = 'Data Load Complete!', frum = section_flag, save_flag = True)"
      ],
      "metadata": {
        "id": "3ph5kkDMPHYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Partitioning/Preprocessing\n",
        "\n",
        "New approach: begin by first grouping by the number of flight legs. Since this implicitly partitions the data by feature space dimension, an intuitive next step would be to train one model per partition. One must be careful to check that the distribution of partition sizes is appropriately balanced when employing this strategy."
      ],
      "metadata": {
        "id": "Sb2VC-HGPkJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procedure"
      ],
      "metadata": {
        "id": "v4MuUN57XOAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_time()\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.ml.tuning import TrainValidationSplit\n",
        "from pyspark.ml.regression import *\n",
        "from pyspark.ml.feature import *\n",
        "time_flag()"
      ],
      "metadata": {
        "id": "aTGXLBgcc9xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_time()\n",
        "# Count flight legs per entry.\n",
        "nL = udf(lambda x: len(x.split('||')))\n",
        "qcols = ['\"' + c + '\"' for c in df.columns]\n",
        "flights_w_legs = eval(f\"df.select({', '.join(qcols)}, nL('segmentsDistance').cast('int').alias('legs'))\")\n",
        "per_leg_stats = flights_w_legs.groupby('legs').count()\n",
        "per_leg_stats.show()\n",
        "time_flag()"
      ],
      "metadata": {
        "id": "1q0QPqNEBCKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_time()\n",
        "# Only interested in flights that are not incredibly rare with respect to number\n",
        "# of legs. Given the extensive bulk of our dataset, we can use a relatively\n",
        "# generous threshold, e.g., 0.05 * df.count() / (per_leg_stats.count() - 1)\n",
        "\n",
        "thresh = 0.05 * df.count() / (per_leg_stats.count() - 1)\n",
        "common_wrt_legs = list(per_leg_stats.where(col('count') > thresh).select('legs').toPandas()['legs'])\n",
        "time_flag()"
      ],
      "metadata": {
        "id": "-4ZZ4y_Rilai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_time()\n",
        "# Train-test split\n",
        "train_df, test_df = df.randomSplit([0.75, 0.25], 42)\n",
        "time_flag()"
      ],
      "metadata": {
        "id": "dOBpw2Zy3h7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_time()\n",
        "# Get leg-based partitions for train/test sets...\n",
        "lbp_train = {}\n",
        "lbp_test = {}\n",
        "\n",
        "flights_w_legs_train = eval(f\"train_df.select({', '.join(qcols)}, nL('segmentsDistance').cast('int').alias('legs'))\")\n",
        "flights_w_legs_test = eval(f\"test_df.select({', '.join(qcols)}, nL('segmentsDistance').cast('int').alias('legs'))\")\n",
        "\n",
        "for i in common_wrt_legs:\n",
        "  lbp_train[i] = flights_w_legs_train.where(col('legs') == i)\n",
        "  lbp_test[i] = flights_w_legs_test.where(col('legs') == i)\n",
        "\n",
        "# Data has now been officially partitioned/filtered!\n",
        "time_flag()"
      ],
      "metadata": {
        "id": "igSu0aaUz_Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_time()\n",
        "# Define abbreviations for the leg-based features.\n",
        "feat_abbs = dict(zip(['segmentsDepartureTimeRaw', 'segmentsArrivalTimeRaw',\n",
        "                      'segmentsArrivalAirportCode', 'segmentsDepartureAirportCode',\n",
        "                     'segmentsAirlineCode', 'segmentsDurationInSeconds', 'segmentsDistance'],\n",
        "                      ['sDTR_', 'sATR_', 'sAAC_', 'sDAC_', 'sAC_', 'sDIS_', 'sD_']))\n",
        "feat_others = ['searchDate', 'flightDate', 'isBasicEconomy', 'seatsRemaining']\n",
        "targets = ['baseFare', 'totalFare']\n",
        "\n",
        "def leg_breaker_maker(delim, n):\n",
        "  return udf(lambda x: x.split(delim)[n])\n",
        "\n",
        "plc_train = {}\n",
        "plc_test = {}\n",
        "for i in common_wrt_legs:\n",
        "  tmp = []\n",
        "  for j in feat_abbs:\n",
        "    tmp.append(\", \".join([f'leg_breaker_maker(\"||\", {k})(col(\"{j}\").alias(str({k})))' for k in range(i)]))\n",
        "  plc_train[i] = (eval(f'lbp_train[{i}].select([{\", \".join(tmp)}] + feat_others + targets)'))\n",
        "  plc_train[i] = eval(f'plc_train[{i}].withColumnsRenamed(dict(zip(plc_train[{i}].columns, ' +\n",
        "                      f'np.concatenate([[feat_abbs[a] + str(k + 1) for k in range({i})] for a in feat_abbs]))))')\n",
        "  plc_test[i] = (eval(f'lbp_test[{i}].select([{\", \".join(tmp)}] + feat_others + targets)'))\n",
        "  plc_test[i] = eval(f'plc_test[{i}].withColumnsRenamed(dict(zip(plc_test[{i}].columns, ' +\n",
        "                      f'np.concatenate([[feat_abbs[a] + str(k + 1) for k in range({i})] for a in feat_abbs]))))')\n",
        "time_flag()"
      ],
      "metadata": {
        "id": "1dGw4KLsRYJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_time()\n",
        "for i in common_wrt_legs:\n",
        "  plc_train[i].show()\n",
        "time_flag()"
      ],
      "metadata": {
        "id": "JaU5_-RQ59Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_time()\n",
        "# Upon preliminary preprocessing, we can make the following observations:\n",
        "\n",
        "# sAAC, sDAC, sAC, isBasicEconomy are categorical variables--these are all to be\n",
        "# one-hot encoded.\n",
        "\n",
        "# sDIS, sD, seatsRemaining are numerical variables--these are only to be cast to\n",
        "# numerical data types.\n",
        "\n",
        "# sDTR/sATR, searchDate, flightDate are date varibles--from these, we can\n",
        "# extract more nuanced features such as times of day (TOD) for\n",
        "# departures/arrivals, days till flight (DTF) between search and flight dates,\n",
        "# and days of weeks (DOY) and months (MOY) of flight dates.\n",
        "\n",
        "rp_train = {}\n",
        "rp_test = {}\n",
        "cat_feats = {}\n",
        "num_feats = {}\n",
        "date_feats = {}\n",
        "\n",
        "# Prepare lists of names for one-hot encoding of categorical features.\n",
        "cat_feats_si = {}\n",
        "cat_feats_ohe = {}\n",
        "\n",
        "for i in common_wrt_legs:\n",
        "  cat_feats[i] = ['isBasicEconomy'] + [c for c in plc_train[i].columns if c.startswith(('sAAC_', 'sDAC_', 'sAC_'))]\n",
        "  num_feats[i] = ['seatsRemaining'] + [c for c in plc_train[i].columns if c.startswith(('sDIS_', 'sD_'))]\n",
        "  date_feats[i] = ['searchDate', 'flightDate'] + [c for c in plc_train[i].columns if c.startswith(('sDTR_', 'sATR_'))]\n",
        "\n",
        "  # Fill lists of names for one-hot encoding of categorical features.\n",
        "  cat_feats_si[i] = [f + '_si' for f in cat_feats[i]]\n",
        "  cat_feats_ohe[i] = [f + '_ohe' for f in cat_feats[i]]\n",
        "\n",
        "  # Cast numerical features to numerical data types, and extract select\n",
        "  # date-based features (TOD will be forgone due to the ambiguity of timezones\n",
        "  # in the dataset).\n",
        "  rp_train[i] = plc_train[i].select(cat_feats[i] +\n",
        "   [plc_train[i][f].cast('float') for f in num_feats[i] + targets] +\n",
        "    [datediff('flightDate', 'searchDate').alias('DTF')] +\n",
        "     [dayofweek('flightDate').alias('DOW')] +\n",
        "      [month('flightDate').alias('MOY')])\n",
        "  rp_test[i] = plc_test[i].select(cat_feats[i] +\n",
        "   [plc_test[i][f].cast('float') for f in num_feats[i] + targets] +\n",
        "    [datediff('flightDate', 'searchDate').alias('DTF')] +\n",
        "     [dayofweek('flightDate').alias('DOW')] +\n",
        "      [month('flightDate').alias('MOY')])\n",
        "\n",
        "time_flag()"
      ],
      "metadata": {
        "id": "SwaijkYvRyie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "(View runtime of procedure following execution)"
      ],
      "metadata": {
        "id": "3CIc1260RZis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "section_flag = time_flag(note = 'Data Partitioning Complete!', frum = section_flag, save_flag = True)"
      ],
      "metadata": {
        "id": "10axBtpzRfhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partition Selection\n",
        "It is recommended to evaluate a small number of partitions (e.g., one or two) at a time due to computational constraints."
      ],
      "metadata": {
        "id": "OEgvSXZkE3O6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procedure"
      ],
      "metadata": {
        "id": "DLKgqziyFbmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select whichever partitions are desired for evaluation.\n",
        "pnos = [1]\n",
        "fp_train = dict(zip(pnos, [rp_train[p] for p in pnos]))\n",
        "fp_test = dict(zip(pnos, [rp_test[p] for p in pnos]))"
      ],
      "metadata": {
        "id": "76C66kENEIGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "(View runtime of procedure following execution)"
      ],
      "metadata": {
        "id": "BSpAtFVtFdsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "section_flag = time_flag(note = 'Partition Selection Complete!', frum = section_flag, save_flag = True)"
      ],
      "metadata": {
        "id": "HAuPGZxkFnma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pipeline"
      ],
      "metadata": {
        "id": "XgVQ0k9IrhKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procedure"
      ],
      "metadata": {
        "id": "HyD_ogP6W9ZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-PCA pipeline.\n",
        "fe_date_feats = ['DTF', 'DOW', 'MOY']\n",
        "\n",
        "success = True\n",
        "!mkdir -p PrePCA\n",
        "pipeline_prefs = {'new': True, 'load_paths': None, 'save_new': True,\n",
        "                  'save_paths': dict(zip(common_wrt_legs,\n",
        "                   [f'/content/PrePCA/pipeline_{i}' for i in common_wrt_legs]))}\n",
        "pre_pipelines = {}\n",
        "train_fes = {}\n",
        "test_fes = {}\n",
        "\n",
        "for i, j in enumerate(fp_train):\n",
        "  log_time()\n",
        "  if pipeline_prefs['new']:\n",
        "    print(f'Fitting pipeline to data partition {i + 1} of {len(fp_train)}...')\n",
        "    pre_pipelines[j] = Pipeline(stages = [Imputer(strategy = 'median',\n",
        "                                                  inputCols = num_feats[j],\n",
        "                                                  outputCols = num_feats[j]),\n",
        "                                          StringIndexer(inputCols = cat_feats[j],\n",
        "                                                        outputCols = cat_feats_si[j],\n",
        "                                                        handleInvalid = 'keep'),\n",
        "                                          OneHotEncoder(inputCols = cat_feats_si[j],\n",
        "                                                        outputCols = cat_feats_ohe[j],\n",
        "                                                        handleInvalid = 'keep'),\n",
        "                                          VectorAssembler(inputCols = num_feats[j] +\n",
        "                                                          cat_feats_ohe[j] +\n",
        "                                                          fe_date_feats,\n",
        "                                                          outputCol = 'vaf'),\n",
        "                                          StandardScaler(inputCol = 'vaf',\n",
        "                                                         outputCol = 'ssf')]).fit(fp_train[j])\n",
        "    if pipeline_prefs['save_new']:\n",
        "      pre_pipelines[j].write().overwrite().save(pipeline_prefs['save_paths'][j])\n",
        "      !tar czf prePCA_part_{j}.tar.gz {pipeline_prefs['save_paths'][j][9:]}\n",
        "  else:\n",
        "    try:\n",
        "      print(f'Retrieving previously fitted pipeline stored in {pipeline_prefs[\"load_paths\"]}...')\n",
        "      !tar xf prePCA_part_{j}.tar.gz\n",
        "      pre_pipelines[j] = PipelineModel.load(pipeline_prefs['load_paths'][j])\n",
        "    except:\n",
        "      print(f'Failed to retrieve pipeline from {pipeline_prefs[\"load_paths\"]}!')\n",
        "      success = False\n",
        "      break\n",
        "\n",
        "  time_flag()\n",
        "\n",
        "  print(f'Running data partition {i + 1} of {len(fp_train)} through pipeline...')\n",
        "  log_time()\n",
        "  train_fes[j] = pre_pipelines[j].transform(fp_train[j])\n",
        "  test_fes[j] = pre_pipelines[j].transform(fp_test[j])\n",
        "  time_flag(note = f'Successfully ran data partition {i + 1} of {len(fp_train)} through pipeline!')\n",
        "\n",
        "if success:\n",
        "  cell_flag = time_flag(note = 'ALL DATA PARTITIONS have been run through pipeline!!!',\n",
        "                        frum = section_flag, save_flag = True)"
      ],
      "metadata": {
        "id": "ndqa9HXNrFjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate PCA stats.\n",
        "success = True\n",
        "!mkdir -p PCA\n",
        "pca_stats_prefs = {'new': True, 'load_paths': None, 'save_new': True,\n",
        "                   'save_paths': dict(zip(common_wrt_legs,\n",
        "                    [f'/content/PCA/stats_{i}.npy' for i in common_wrt_legs]))}\n",
        "vecLen = udf(lambda x: len(x))\n",
        "raw_feat_space_dims = {}\n",
        "pca_stats = {}\n",
        "\n",
        "for i, j in enumerate(train_fes):\n",
        "  log_time()\n",
        "  if pca_stats_prefs['new']:\n",
        "    print(f'Verifying raw feature space dimension for data partition {i + 1} of {len(train_fes)}...')\n",
        "\n",
        "    # Get length of assembled vector for evaluation of PCA.\n",
        "    tmp_ssf_len = train_fes[j].select(vecLen('ssf').cast('float')).distinct().toPandas()\n",
        "    assert(tmp_ssf_len.size == 1)\n",
        "    raw_feat_space_dims[j] = list(tmp_ssf_len.iloc[0])[0]\n",
        "    time_flag(note = 'Raw feature space dimension verified.')\n",
        "\n",
        "    print('Proceeding to generate PCA stats...')\n",
        "    log_time()\n",
        "\n",
        "    # Now evaluate PCA.\n",
        "    pca_stats[j] = PCA(k = raw_feat_space_dims[j], inputCol = 'ssf',\n",
        "                       outputCol = 'pcf').fit(train_fes[j]).explainedVariance.values\n",
        "\n",
        "    if pca_stats_prefs['save_new']:\n",
        "      np.save(pca_stats_prefs['save_paths'][j], pca_stats[j])\n",
        "  else:\n",
        "    try:\n",
        "      print(f'Retrieving previously generated PCA stats stored in {pca_stats_prefs[\"load_paths\"]}...')\n",
        "      pca_stats[j] = np.load(pca_stats_prefs['load_paths'][j])\n",
        "    except:\n",
        "      print(f'Failed to retrieve PCA stats from {pca_stats_prefs[\"load_paths\"]}!')\n",
        "      success = False\n",
        "      break\n",
        "\n",
        "  time_flag(note = f'PCA stats obtained for {i + 1} / {len(train_fes)} of the data partitions...')\n",
        "\n",
        "if success:\n",
        "  cell_flag = time_flag(note = 'PCA stats generated for ALL DATA PARTITIONS!!!',\n",
        "                        frum = cell_flag, save_flag = True)"
      ],
      "metadata": {
        "id": "9SV-G4aW3VxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_time()\n",
        "# Impose an arbitrary threshold on the minimum fraction of variance to be\n",
        "# preserved in the reduced-dimension space, and compute the corresponding\n",
        "# minimum reduced dimension.\n",
        "rds_var_thresh = 0.95\n",
        "\n",
        "pca_ks = {}\n",
        "for i, j in enumerate(pca_stats):\n",
        "  pca_ks[j] = np.argmax(np.cumsum(pca_stats[j]) > rds_var_thresh) + 1\n",
        "\n",
        "cell_flag = time_flag(save_flag = True)"
      ],
      "metadata": {
        "id": "g5SvDGuzfq7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Proceed to perform the appropriate PCA.\n",
        "\n",
        "success = True\n",
        "pca_prefs = {'new': True, 'load_paths': None, 'save_new': True,\n",
        "             'save_paths': dict(zip(common_wrt_legs,\n",
        "              [f'/content/PCA/pipeline_{i}' for i in common_wrt_legs]))}\n",
        "pca_mods = {}\n",
        "train_pca = {}\n",
        "test_pca = {}\n",
        "for i, j in enumerate(pca_ks):\n",
        "  log_time()\n",
        "  if pca_prefs['new']:\n",
        "    print(f'Generating PCA model for data partition {i + 1} of {len(pca_ks)}...')\n",
        "    pca_mods[j] = PCA(k = pca_ks[j], inputCol = 'ssf', outputCol = 'pcf').fit(train_fes[j])\n",
        "    if pca_prefs['save_new']:\n",
        "      pca_mods[j].save(pca_prefs['save_paths'][j])\n",
        "      !tar czf PCA_part_{j}.tar.gz {pca_prefs['save_paths'][j][9:]}\n",
        "  else:\n",
        "    try:\n",
        "      print(f'Retrieving previously trained PCA model stored in {pca_prefs[\"load_paths\"]}...')\n",
        "      !tar xf PCA_part_{j}.tar.gz\n",
        "      pca_mods[j] = PCAModel.load(pca_prefs['load_paths'][j])\n",
        "    except:\n",
        "      print(f'Failed to retrieve PCA stats from {pca_prefs[\"load_paths\"]}!')\n",
        "      success = False\n",
        "      break\n",
        "  time_flag()\n",
        "\n",
        "  print(f'Proceeding to reduce dimension associated with data partition {i + 1} of {len(pca_ks)}...')\n",
        "\n",
        "  log_time()\n",
        "  train_pca[j] = pca_mods[j].transform(train_fes[j])\n",
        "  test_pca[j] = pca_mods[j].transform(test_fes[j])\n",
        "  time_flag(f'Successfully reduced dimension associated with data partition {i + 1} of {len(pca_ks)}!')\n",
        "\n",
        "if success:\n",
        "  time_flag(note = 'PCA performed on ALL DATA PARTITIONS!!!', frum = cell_flag)"
      ],
      "metadata": {
        "id": "NAtTnWfxyr8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "(View runtime of procedure following execution)"
      ],
      "metadata": {
        "id": "nwaA2gLNRmG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "section_flag = time_flag(note = 'Data Pipeline Fitting Complete!', frum = section_flag, save_flag = True)"
      ],
      "metadata": {
        "id": "v5VP40yiSEcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "M74AUReu9i8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procedure"
      ],
      "metadata": {
        "id": "lf3grmISVuwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_time()\n",
        "# Choose target: either baseFare or totalFare\n",
        "target_choice = 'baseFare'\n",
        "time_flag()"
      ],
      "metadata": {
        "id": "f0zq3UDl95FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train using a PySpark-compatible model class of choice.\n",
        "success = True\n",
        "mod_prefs = {'new': True, 'load_paths': None, 'save_new': False, 'save_paths': None}\n",
        "mod_class = {'name': 'linear regressor', 'class': LinearRegression, 'mod': LinearRegressionModel}\n",
        "mods = {}\n",
        "train_res = {}\n",
        "test_res = {}\n",
        "for i, j in enumerate(train_pca):\n",
        "  log_time()\n",
        "  if mod_prefs['new']:\n",
        "    print(f'Training a {mod_class[\"name\"]} on data partition {i + 1} of {len(train_pca)}...')\n",
        "    mods[j] = mod_class['class'](featuresCol = 'pcf', labelCol = target_choice).fit(train_pca[j])\n",
        "    if mod_prefs['save_new']:\n",
        "      mods[j].save(mod_prefs['save_paths'][j])\n",
        "      !tar czf Models_part_{j}.tar.gz {mod_prefs['save_paths'][j][9:]}\n",
        "  else:\n",
        "    try:\n",
        "      print(f'Retrieving previously trained model stored in {mod_prefs[\"load_paths\"]}...')\n",
        "      !tar xf Models_part_{j}.tar.gz\n",
        "      mods[j] = mod_class['mod'].load(mod_prefs['load_paths'][j])\n",
        "    except:\n",
        "      print(f'Failed to retrieve models from {mod_prefs[\"load_paths\"]}!')\n",
        "      success = False\n",
        "      break\n",
        "  time_flag()\n",
        "\n",
        "  print(f'Proceeding to generate predictions for data partition {i + 1} of {len(train_pca)}...')\n",
        "\n",
        "  log_time()\n",
        "  train_res[j] = mods[j].transform(train_pca[j])\n",
        "  test_res[j] = mods[j].transform(test_pca[j])\n",
        "  time_flag(note = f'Predictions generated for data partition {i + 1} of {len(train_pca)}...')\n",
        "\n",
        "if success:\n",
        "  cell_flag = time_flag(note = 'Model training/prediction generation complete for ALL DATA PARTITIONS!!!',\n",
        "                        frum = section_flag, save_flag = True)"
      ],
      "metadata": {
        "id": "75siW2g_9k4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve results.\n",
        "\n",
        "train_SE = {}\n",
        "train_SSE = {}\n",
        "train_C = {}\n",
        "test_SE = {}\n",
        "test_SSE = {}\n",
        "test_C = {}\n",
        "\n",
        "for i, j in enumerate(pnos):\n",
        "  print(f'Evaluating model associated with data partition {i + 1} of {len(pnos)}...')\n",
        "  log_time()\n",
        "\n",
        "  train_SE[j] = train_res[j].select(((train_res[j][target_choice] - train_res[j]['prediction']) ** 2).alias('SE'))\n",
        "  train_SSE[j] = list(train_SE[j].agg({'SE': 'sum'}).toPandas().iloc[0])[0]\n",
        "  train_C[j] = list(train_SE[j].agg({'SE': 'count'}).toPandas().iloc[0])[0]\n",
        "  test_SE[j] = test_res[j].select(((test_res[j][target_choice] - test_res[j]['prediction']) ** 2).alias('SE'))\n",
        "  test_SSE[j] = list(test_SE[j].agg({'SE': 'sum'}).toPandas().iloc[0])[0]\n",
        "  test_C[j] = list(test_SE[j].agg({'SE': 'count'}).toPandas().iloc[0])[0]\n",
        "  time_flag()\n",
        "\n",
        "time_flag(note = 'Model evaluation complete for ALL DATA PARTITIONS!!!', frum = cell_flag)"
      ],
      "metadata": {
        "id": "lM8aq2WIYY2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_time()\n",
        "# Assess linear regression results.\n",
        "\n",
        "print(f'Train sum of squared errors for {mod_class[\"name\"]}: {np.sum(list(train_SSE.values()))}')\n",
        "print(f'Number of train observations: {np.sum(list(train_C.values()))}')\n",
        "print(f'Test sum of squared errors for {mod_class[\"name\"]}: {np.sum(list(test_SSE.values()))}')\n",
        "print(f'Number of test observations: {np.sum(list(test_C.values()))}')\n",
        "time_flag()"
      ],
      "metadata": {
        "id": "do1E9x4luRIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "(View runtime of procedure following execution)"
      ],
      "metadata": {
        "id": "gEFi6tlxSPR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "section_flag = time_flag(note = 'Model Training Complete!', frum = section_flag, save_flag = True)"
      ],
      "metadata": {
        "id": "-wgyKYuvShet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook Summary: Total Runtime"
      ],
      "metadata": {
        "id": "tQp_HWNsSHhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_flag(note = 'ENTIRE NOTEBOOK EXECUTION COMPLETE!!!!!', frum = 0)"
      ],
      "metadata": {
        "id": "Exi_v9oq2Wa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time.sleep(12 * 60 * 60)"
      ],
      "metadata": {
        "id": "2iwdN3tuOBZD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9yymT4zVRTTU",
        "RXdFgw9ZXijn",
        "hDPuZNYLPh6u",
        "v4MuUN57XOAh",
        "3CIc1260RZis",
        "OEgvSXZkE3O6",
        "DLKgqziyFbmG",
        "BSpAtFVtFdsi",
        "XgVQ0k9IrhKR",
        "HyD_ogP6W9ZU",
        "nwaA2gLNRmG5",
        "M74AUReu9i8M",
        "lf3grmISVuwQ",
        "gEFi6tlxSPR_",
        "tQp_HWNsSHhP"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}