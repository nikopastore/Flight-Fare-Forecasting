{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " # Flight Prices XGBoost with Dask (GPU) Notebook\n",
        "\n",
        " This notebook demonstrates how to:\n",
        " - Download and extract the flight prices dataset.\n",
        " - Set up a local Dask cluster.\n",
        " - Read and process the data using Dask.\n",
        " - Prepare features and target for regression.\n",
        " - Train an XGBoost model using GPU acceleration via Dask.\n",
        " - Evaluate the model by computing the RMSE in a distributed manner.\n",
        "\n",
        " **Note:*  Ensure that you have a compatible GPU setup for GPU acceleration."
      ],
      "metadata": {
        "id": "mKJ_1absfYYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Section 1: Download and Setup\n",
        "\n",
        " Install required packages, download the dataset from Kaggle, and unzip it."
      ],
      "metadata": {
        "id": "dQkw497Mf_jV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install xgboost --upgrade\n",
        "!pip install kaggle\n",
        "!kaggle datasets download -d dilwong/flightprices\n",
        "!unzip -n flightprices.zip\n",
        "!pip install \"dask[complete]==2024.10.0\" xgboost --upgrade"
      ],
      "metadata": {
        "id": "k2kG2NdTfijf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b50a7b-87a7-4f32-cda2-4a13df869131"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.10)\n",
            "Dataset URL: https://www.kaggle.com/datasets/dilwong/flightprices\n",
            "License(s): Attribution 4.0 International (CC BY 4.0)\n",
            "Downloading flightprices.zip to /content\n",
            "100% 5.51G/5.51G [02:17<00:00, 44.2MB/s]\n",
            "100% 5.51G/5.51G [02:17<00:00, 42.9MB/s]\n",
            "Archive:  flightprices.zip\n",
            "  inflating: itineraries.csv         \n",
            "Requirement already satisfied: dask==2024.10.0 in /usr/local/lib/python3.11/dist-packages (from dask[complete]==2024.10.0) (2024.10.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask==2024.10.0->dask[complete]==2024.10.0) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask==2024.10.0->dask[complete]==2024.10.0) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask==2024.10.0->dask[complete]==2024.10.0) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dask==2024.10.0->dask[complete]==2024.10.0) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask==2024.10.0->dask[complete]==2024.10.0) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask==2024.10.0->dask[complete]==2024.10.0) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask==2024.10.0->dask[complete]==2024.10.0) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask==2024.10.0->dask[complete]==2024.10.0) (8.6.1)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.11/dist-packages (from dask[complete]==2024.10.0) (17.0.0)\n",
            "Collecting lz4>=4.3.2 (from dask[complete]==2024.10.0)\n",
            "  Downloading lz4-4.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->dask==2024.10.0->dask[complete]==2024.10.0) (3.21.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask==2024.10.0->dask[complete]==2024.10.0) (1.0.0)\n",
            "Requirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.11/dist-packages (from dask==2024.10.0->dask[complete]==2024.10.0) (2.2.2)\n",
            "Collecting dask-expr<1.2,>=1.1 (from dask==2024.10.0->dask[complete]==2024.10.0)\n",
            "  Downloading dask_expr-1.1.21-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: bokeh>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from dask==2024.10.0->dask[complete]==2024.10.0) (3.6.3)\n",
            "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.11/dist-packages (from dask==2024.10.0->dask[complete]==2024.10.0) (3.1.5)\n",
            "Collecting distributed==2024.10.0 (from dask==2024.10.0->dask[complete]==2024.10.0)\n",
            "  Downloading distributed-2024.10.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from distributed==2024.10.0->dask==2024.10.0->dask[complete]==2024.10.0) (1.1.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from distributed==2024.10.0->dask==2024.10.0->dask[complete]==2024.10.0) (5.9.5)\n",
            "Collecting sortedcontainers>=2.0.5 (from distributed==2024.10.0->dask==2024.10.0->dask[complete]==2024.10.0)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting tblib>=1.6.0 (from distributed==2024.10.0->dask==2024.10.0->dask[complete]==2024.10.0)\n",
            "  Downloading tblib-3.0.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from distributed==2024.10.0->dask==2024.10.0->dask[complete]==2024.10.0) (6.4.2)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.11/dist-packages (from distributed==2024.10.0->dask==2024.10.0->dask[complete]==2024.10.0) (2.3.0)\n",
            "Collecting zict>=3.0.0 (from distributed==2024.10.0->dask==2024.10.0->dask[complete]==2024.10.0)\n",
            "  Downloading zict-3.0.0-py2.py3-none-any.whl.metadata (899 bytes)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1.0->dask==2024.10.0->dask[complete]==2024.10.0) (1.3.1)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1.0->dask==2024.10.0->dask[complete]==2024.10.0) (11.1.0)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1.0->dask==2024.10.0->dask[complete]==2024.10.0) (2025.1.0)\n",
            "INFO: pip is looking at multiple versions of dask-expr to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting dask-expr<1.2,>=1.1 (from dask==2024.10.0->dask[complete]==2024.10.0)\n",
            "  Downloading dask_expr-1.1.20-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.19-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.18-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.16-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.10.3->dask==2024.10.0->dask[complete]==2024.10.0) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->dask==2024.10.0->dask[complete]==2024.10.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->dask==2024.10.0->dask[complete]==2024.10.0) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->dask==2024.10.0->dask[complete]==2024.10.0) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask==2024.10.0->dask[complete]==2024.10.0) (1.17.0)\n",
            "Downloading lz4-4.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distributed-2024.10.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dask_expr-1.1.16-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading tblib-3.0.0-py3-none-any.whl (12 kB)\n",
            "Downloading zict-3.0.0-py2.py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sortedcontainers, zict, tblib, lz4, distributed, dask-expr\n",
            "Successfully installed dask-expr-1.1.16 distributed-2024.10.0 lz4-4.4.3 sortedcontainers-2.4.0 tblib-3.0.0 zict-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Section 2: Import Libraries and Initialize Dask Cluster\n",
        "\n",
        " Import necessary libraries and create a local Dask cluster."
      ],
      "metadata": {
        "id": "cJ4jsuZlgGDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "from dask.distributed import Client, LocalCluster\n",
        "\n",
        "# Create a local Dask cluster with 1 worker and 8 threads per worker\n",
        "cluster = LocalCluster(n_workers=1, threads_per_worker=8)\n",
        "client = Client(cluster)\n",
        "print(client)"
      ],
      "metadata": {
        "id": "g-i_saOWgL7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffe74a09-970a-4797-da27-955827ece7eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.http.proxy:To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:38245\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:8787/status\n",
            "INFO:distributed.scheduler:Registering Worker plugin shuffle\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:42275'\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:37789', name: 0, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:37789\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:41294\n",
            "INFO:distributed.scheduler:Receive client connection: Client-6cb8f85f-f174-11ef-8195-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:41310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Client: 'tcp://127.0.0.1:38245' processes=1 threads=8, memory=50.99 GiB>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Section 3: Data Loading and Overview\n",
        "\n",
        " Read the CSV file using Dask. The blocksize parameter defines the size of each block (e.g., 64MB).\n",
        " Display a preview of the data and show the number of partitions."
      ],
      "metadata": {
        "id": "aiGWPnbkgPvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = dd.read_csv('itineraries.csv', blocksize=64e6, assume_missing=True)\n",
        "print(df.head())\n",
        "print(\"Number of partitions:\", df.npartitions)"
      ],
      "metadata": {
        "id": "6XKPIYtogUtp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d53b832-fcea-4179-ed25-fc40b84cdd72"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                              legId  searchDate  flightDate startingAirport  \\\n",
            "0  9ca0e81111c683bec1012473feefd28f  2022-04-16  2022-04-17             ATL   \n",
            "1  98685953630e772a098941b71906592b  2022-04-16  2022-04-17             ATL   \n",
            "2  98d90cbc32bfbb05c2fc32897c7c1087  2022-04-16  2022-04-17             ATL   \n",
            "3  969a269d38eae583f455486fa90877b4  2022-04-16  2022-04-17             ATL   \n",
            "4  980370cf27c89b40d2833a1d5afc9751  2022-04-16  2022-04-17             ATL   \n",
            "\n",
            "  destinationAirport fareBasisCode travelDuration  elapsedDays  \\\n",
            "0                BOS      LA0NX0MC        PT2H29M          0.0   \n",
            "1                BOS      LA0NX0MC        PT2H30M          0.0   \n",
            "2                BOS      LA0NX0MC        PT2H30M          0.0   \n",
            "3                BOS      LA0NX0MC        PT2H32M          0.0   \n",
            "4                BOS      LA0NX0MC        PT2H34M          0.0   \n",
            "\n",
            "   isBasicEconomy  isRefundable  ...  segmentsArrivalTimeEpochSeconds  \\\n",
            "0           False         False  ...                       1650223560   \n",
            "1           False         False  ...                       1650200400   \n",
            "2           False         False  ...                       1650218700   \n",
            "3           False         False  ...                       1650227460   \n",
            "4           False         False  ...                       1650213180   \n",
            "\n",
            "          segmentsArrivalTimeRaw  segmentsArrivalAirportCode  \\\n",
            "0  2022-04-17T15:26:00.000-04:00                         BOS   \n",
            "1  2022-04-17T09:00:00.000-04:00                         BOS   \n",
            "2  2022-04-17T14:05:00.000-04:00                         BOS   \n",
            "3  2022-04-17T16:31:00.000-04:00                         BOS   \n",
            "4  2022-04-17T12:33:00.000-04:00                         BOS   \n",
            "\n",
            "   segmentsDepartureAirportCode  segmentsAirlineName segmentsAirlineCode  \\\n",
            "0                           ATL                Delta                  DL   \n",
            "1                           ATL                Delta                  DL   \n",
            "2                           ATL                Delta                  DL   \n",
            "3                           ATL                Delta                  DL   \n",
            "4                           ATL                Delta                  DL   \n",
            "\n",
            "  segmentsEquipmentDescription segmentsDurationInSeconds segmentsDistance  \\\n",
            "0                  Airbus A321                      8940              947   \n",
            "1                  Airbus A321                      9000              947   \n",
            "2               Boeing 757-200                      9000              947   \n",
            "3                  Airbus A321                      9120              947   \n",
            "4                  Airbus A321                      9240              947   \n",
            "\n",
            "  segmentsCabinCode  \n",
            "0             coach  \n",
            "1             coach  \n",
            "2             coach  \n",
            "3             coach  \n",
            "4             coach  \n",
            "\n",
            "[5 rows x 27 columns]\n",
            "Number of partitions: 485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Section 4: Data Preprocessing\n",
        "\n",
        " Define the features and target, convert relevant columns to float32 to optimize memory, and fill missing values using the approximate median.\n",
        " Optionally, you can save the DataFrame in Parquet format for faster future access."
      ],
      "metadata": {
        "id": "aCQOdhKEgYka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the features and target variable\n",
        "features = ['elapsedDays', 'totalTravelDistance', 'seatsRemaining']\n",
        "target = 'baseFare'\n",
        "\n",
        "# Convert specified columns to float32\n",
        "for col in features + [target]:\n",
        "    df[col] = df[col].astype('float32')\n",
        "\n",
        "# Fill missing values for features using the approximate median\n",
        "for col in features:\n",
        "    med = df[col].median_approximate()\n",
        "    df[col] = df[col].fillna(med)\n",
        "df[target] = df[target].fillna(df[target].median_approximate())\n",
        "\n",
        "# Optional: Save to Parquet for faster future access\n",
        "# df.to_parquet('itineraries.parquet')"
      ],
      "metadata": {
        "id": "7I0q_cMtghob"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Section 5: Data Splitting and Conversion to Dask Arrays\n",
        "\n",
        " Split the data into training (80%) and testing (20%) sets. Then, convert the Dask DataFrames into Dask arrays,\n",
        " and create DaskDMatrix objects for XGBoost."
      ],
      "metadata": {
        "id": "kseKHtYygkmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Split the dataset into train (80%) and test (20%) sets\n",
        "train, test = df.random_split([0.8, 0.2], random_state=42)\n",
        "\n",
        "# Convert the Dask DataFrames into Dask arrays\n",
        "X_train = train[features].to_dask_array(lengths=True)\n",
        "y_train = train[target].to_dask_array(lengths=True)\n",
        "X_test = test[features].to_dask_array(lengths=True)\n",
        "y_test = test[target].to_dask_array(lengths=True)\n",
        "\n",
        "# Create DaskDMatrix objects for XGBoost\n",
        "dtrain = xgb.dask.DaskDMatrix(client, X_train, y_train)\n",
        "dtest  = xgb.dask.DaskDMatrix(client, X_test, y_test)"
      ],
      "metadata": {
        "id": "ds-ny-W-gqF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c96bef6e-db59-4d31-e09c-62b9a0364031"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.core:Event loop was unresponsive in Scheduler for 5.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Scheduler for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Scheduler for 4.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Section 6: Model Training with XGBoost using GPU\n",
        "\n",
        " Set the parameters for XGBoost to use GPU acceleration, and train the model using XGBoost's Dask interface."
      ],
      "metadata": {
        "id": "dvLOwUAwgtJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.array as da\n",
        "\n",
        "# XGBoost parameters for GPU usage\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'tree_method': 'gpu_hist',      # Use GPU to build trees\n",
        "    'predictor': 'gpu_predictor',   # Use GPU for predictions\n",
        "    'max_depth': 6,\n",
        "    'eta': 0.1,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "num_rounds = 100\n",
        "\n",
        "# Train the model using the Dask interface of XGBoost\n",
        "output = xgb.dask.train(client, params, dtrain, num_boost_round=num_rounds, evals=[(dtest, 'test')])\n",
        "bst = output['booster']"
      ],
      "metadata": {
        "id": "x_G0JvCVg5GY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8faa4d4a-99a7-480a-f576-4320e3446281"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.worker:Run out-of-band function '_start_tracker'\n",
            "INFO:distributed.scheduler:Receive client connection: Client-worker-bd28a9a6-f184-11ef-8820-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:56128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Section 7: Prediction and Evaluation\n",
        "\n",
        " Make predictions on the test set using the trained model and calculate the RMSE in a distributed manner."
      ],
      "metadata": {
        "id": "mBT5bBLHg8fv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MSjirTd9fSXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01eb6da4-2975-40dd-cfda-18f72e06d71a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.core:Event loop was unresponsive in Nanny for 4.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
            "INFO:distributed.core:Event loop was unresponsive in Scheduler for 4.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with GPU (Dask) - RMSE: 147.79932\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Make predictions on the test set\n",
        "preds = xgb.dask.predict(client, bst, dtest)\n",
        "\n",
        "# Compute RMSE using Dask Array operations\n",
        "rmse = da.sqrt(((y_test - preds) ** 2).mean())\n",
        "print(\"XGBoost with GPU (Dask) - RMSE:\", rmse.compute())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End of Notebook\n",
        "\n",
        " This notebook showcased how to:\n",
        " - Set up and use a local Dask cluster.\n",
        " - Process and prepare the flight prices dataset with Dask.\n",
        " - Train an XGBoost model using GPU acceleration.\n",
        " - Evaluate model performance using distributed computations."
      ],
      "metadata": {
        "id": "LXzoXNjbhCVT"
      }
    }
  ]
}